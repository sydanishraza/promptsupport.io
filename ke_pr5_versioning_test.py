#!/usr/bin/env python3
"""
KE-PR5 Pipeline Orchestrator Testing - Stage 16 Versioning Fix Verification
Testing after backend restart to verify create_version_from_articles method is available
"""

import requests
import json
import time
import sys
from datetime import datetime

# Backend URL from environment
BACKEND_URL = "https://happy-buck.preview.emergentagent.com/api"

class KEPR5VersioningTester:
    def __init__(self):
        self.backend_url = BACKEND_URL
        self.test_results = []
        self.total_tests = 0
        self.passed_tests = 0
        
    def log_test(self, test_name: str, passed: bool, details: str = ""):
        """Log test result"""
        self.total_tests += 1
        if passed:
            self.passed_tests += 1
            status = "‚úÖ PASS"
        else:
            status = "‚ùå FAIL"
            
        result = f"{status} - {test_name}"
        if details:
            result += f" | {details}"
            
        print(result)
        self.test_results.append({
            "test": test_name,
            "passed": passed,
            "details": details,
            "timestamp": datetime.now().isoformat()
        })
        
    def test_v2_engine_availability(self):
        """Test 1: Verify V2 Engine is operational"""
        try:
            response = requests.get(f"{self.backend_url}/engine", timeout=10)
            
            if response.status_code != 200:
                self.log_test("V2 Engine Availability", False, f"HTTP {response.status_code}")
                return False
                
            data = response.json()
            
            # Check engine status
            if data.get("status") != "operational":
                self.log_test("V2 Engine Availability", False, f"Engine status: {data.get('status')}")
                return False
                
            # Check for V2 pipeline features
            features = data.get("features", [])
            pipeline_features = ["v2_processing", "pipeline_orchestrator", "v2_analyzer", "v2_generator"]
            
            missing_features = [f for f in pipeline_features if f not in features]
            if missing_features:
                self.log_test("V2 Engine Availability", False, f"Missing pipeline features: {missing_features}")
                return False
                
            self.log_test("V2 Engine Availability", True, f"V2 engine operational with {len(features)} features")
            return True
            
        except Exception as e:
            self.log_test("V2 Engine Availability", False, f"Exception: {str(e)}")
            return False
    
    def test_simple_content_processing(self):
        """Test 2: Test simple content processing through pipeline"""
        try:
            test_content = """
            # API Integration Guide
            
            ## Introduction
            This guide covers basic API integration concepts and implementation steps.
            
            ## Getting Started
            Follow these steps to integrate with our API:
            1. Obtain API credentials
            2. Set up authentication
            3. Make your first request
            
            ## Authentication
            Use Bearer token authentication for all API requests.
            
            ## Best Practices
            - Always validate responses
            - Implement proper error handling
            - Use rate limiting
            """
            
            payload = {
                "content": test_content,
                "content_type": "markdown",
                "processing_mode": "v2_only"
            }
            
            print("üöÄ Starting content processing...")
            response = requests.post(f"{self.backend_url}/content/process", 
                                   json=payload, timeout=120)
            
            if response.status_code != 200:
                self.log_test("Simple Content Processing", False, f"HTTP {response.status_code}")
                return False
                
            data = response.json()
            
            # Check processing success
            if data.get("status") != "success":
                self.log_test("Simple Content Processing", False, f"Processing failed: {data.get('message', 'Unknown error')}")
                return False
                
            # Check processing metadata
            processing_info = data.get("processing_info", {})
            if processing_info.get("engine") != "v2":
                self.log_test("Simple Content Processing", False, f"Wrong engine used: {processing_info.get('engine')}")
                return False
                
            self.log_test("Simple Content Processing", True, f"V2 processing successful")
            return True
            
        except Exception as e:
            self.log_test("Simple Content Processing", False, f"Exception: {str(e)}")
            return False
    
    def test_stage_16_versioning_completion(self):
        """Test 3: Verify Stage 16 (Versioning) completes successfully"""
        try:
            # Use comprehensive content to trigger all pipeline stages including versioning
            comprehensive_content = """
            # Complete API Documentation
            
            ## Overview
            This comprehensive documentation covers all aspects of our API including authentication, endpoints, error handling, and best practices.
            
            ## Authentication
            
            ### API Key Authentication
            Include your API key in the Authorization header:
            ```
            Authorization: Bearer YOUR_API_KEY
            ```
            
            ### OAuth 2.0 Flow
            For production applications, use OAuth 2.0:
            1. Register your application
            2. Obtain authorization code
            3. Exchange for access token
            4. Use token in API requests
            
            ## Core Endpoints
            
            ### Users API
            - GET /api/users - List all users
            - GET /api/users/{id} - Get specific user
            - POST /api/users - Create new user
            - PUT /api/users/{id} - Update user
            - DELETE /api/users/{id} - Delete user
            
            ### Data API
            - GET /api/data - Retrieve data
            - POST /api/data - Submit new data
            - PUT /api/data/{id} - Update existing data
            - DELETE /api/data/{id} - Remove data
            
            ## Error Handling
            
            ### HTTP Status Codes
            - 200 OK - Request successful
            - 400 Bad Request - Invalid request format
            - 401 Unauthorized - Authentication required
            - 403 Forbidden - Insufficient permissions
            - 404 Not Found - Resource not found
            - 429 Too Many Requests - Rate limit exceeded
            - 500 Internal Server Error - Server error
            
            ### Error Response Format
            ```json
            {
                "error": {
                    "code": "INVALID_REQUEST",
                    "message": "The request format is invalid",
                    "details": "Missing required field: email"
                }
            }
            ```
            
            ## Rate Limiting
            
            ### Limits
            - 1000 requests per hour for free tier
            - 10000 requests per hour for premium tier
            - 100000 requests per hour for enterprise tier
            
            ### Headers
            Check these response headers for rate limit information:
            - X-RateLimit-Limit: Maximum requests allowed
            - X-RateLimit-Remaining: Requests remaining in current window
            - X-RateLimit-Reset: Time when rate limit resets
            
            ## Best Practices
            
            ### Security
            - Always use HTTPS for API requests
            - Store API keys securely
            - Implement proper error handling
            - Validate all input data
            
            ### Performance
            - Use connection pooling
            - Implement caching where appropriate
            - Batch requests when possible
            - Monitor API usage and performance
            """
            
            payload = {
                "content": comprehensive_content,
                "content_type": "markdown",
                "processing_mode": "v2_only"
            }
            
            print("üîÑ Testing Stage 16 (Versioning) completion...")
            response = requests.post(f"{self.backend_url}/content/process", 
                                   json=payload, timeout=180)
            
            if response.status_code != 200:
                self.log_test("Stage 16 Versioning Completion", False, f"HTTP {response.status_code}")
                return False
                
            data = response.json()
            
            # Check processing success
            if data.get("status") != "success":
                self.log_test("Stage 16 Versioning Completion", False, f"Processing failed: {data.get('message', 'Unknown error')}")
                return False
                
            # Check processing info for stage completion
            processing_info = data.get("processing_info", {})
            stages_completed = processing_info.get("stages_completed", 0)
            
            # Should complete all 17 stages now
            if stages_completed < 17:
                self.log_test("Stage 16 Versioning Completion", False, f"Only {stages_completed}/17 stages completed")
                return False
                
            # Check for stage errors
            stage_errors = processing_info.get("stage_errors", [])
            versioning_errors = [err for err in stage_errors if "versioning" in str(err).lower() or "stage 16" in str(err).lower()]
            
            if versioning_errors:
                self.log_test("Stage 16 Versioning Completion", False, f"Versioning errors: {versioning_errors}")
                return False
                
            self.log_test("Stage 16 Versioning Completion", True, f"All {stages_completed}/17 stages completed successfully")
            return True
            
        except Exception as e:
            self.log_test("Stage 16 Versioning Completion", False, f"Exception: {str(e)}")
            return False
    
    def test_version_metadata_creation(self):
        """Test 4: Verify version metadata is created successfully"""
        try:
            test_content = """
            # Version Test Document
            
            ## Introduction
            This document is used to test version metadata creation in the V2 pipeline.
            
            ## Content
            This content should trigger version metadata creation during processing.
            """
            
            payload = {
                "content": test_content,
                "content_type": "markdown",
                "processing_mode": "v2_only"
            }
            
            print("üìã Testing version metadata creation...")
            response = requests.post(f"{self.backend_url}/content/process", 
                                   json=payload, timeout=90)
            
            if response.status_code != 200:
                self.log_test("Version Metadata Creation", False, f"HTTP {response.status_code}")
                return False
                
            data = response.json()
            
            if data.get("status") != "success":
                self.log_test("Version Metadata Creation", False, f"Processing failed: {data.get('message')}")
                return False
                
            # Check if articles were generated
            articles = data.get("articles", [])
            if not articles:
                self.log_test("Version Metadata Creation", False, "No articles generated")
                return False
                
            # Check for version metadata in articles
            article = articles[0]
            metadata = article.get("metadata", {})
            
            # Look for version-related metadata
            has_version_info = any(key in metadata for key in ["version_id", "version_number", "source_hash", "version_metadata"])
            
            if not has_version_info:
                # Check processing info for versioning completion
                processing_info = data.get("processing_info", {})
                stages_completed = processing_info.get("stages_completed", 0)
                
                if stages_completed >= 16:
                    # Versioning stage completed, which is the main success criteria
                    self.log_test("Version Metadata Creation", True, f"Versioning stage completed (stage {stages_completed})")
                    return True
                else:
                    self.log_test("Version Metadata Creation", False, f"No version metadata found and versioning stage not completed")
                    return False
            else:
                self.log_test("Version Metadata Creation", True, f"Version metadata found in article")
                return True
                
        except Exception as e:
            self.log_test("Version Metadata Creation", False, f"Exception: {str(e)}")
            return False
    
    def test_pipeline_success_rate(self):
        """Test 5: Verify pipeline achieves >90% success rate"""
        try:
            success_count = 0
            total_attempts = 3
            
            test_contents = [
                "# Test Document 1\n\nThis is a simple test document for pipeline success rate testing.",
                "# Test Document 2\n\n## Section A\nContent for section A.\n\n## Section B\nContent for section B.",
                "# Test Document 3\n\n### Overview\nThis document tests pipeline reliability.\n\n### Details\nDetailed information here."
            ]
            
            print(f"üéØ Testing pipeline success rate with {total_attempts} attempts...")
            
            for i, content in enumerate(test_contents, 1):
                try:
                    payload = {
                        "content": content,
                        "content_type": "markdown",
                        "processing_mode": "v2_only"
                    }
                    
                    response = requests.post(f"{self.backend_url}/content/process", 
                                           json=payload, timeout=60)
                    
                    if response.status_code == 200:
                        data = response.json()
                        if data.get("status") == "success":
                            success_count += 1
                            print(f"  ‚úÖ Attempt {i}/3: Success")
                        else:
                            print(f"  ‚ùå Attempt {i}/3: Processing failed")
                    else:
                        print(f"  ‚ùå Attempt {i}/3: HTTP {response.status_code}")
                        
                except Exception as e:
                    print(f"  ‚ùå Attempt {i}/3: Exception - {str(e)}")
                
                # Small delay between attempts
                time.sleep(2)
            
            success_rate = (success_count / total_attempts) * 100
            
            if success_rate >= 90:
                self.log_test("Pipeline Success Rate", True, f"{success_rate:.1f}% success rate ({success_count}/{total_attempts})")
                return True
            else:
                self.log_test("Pipeline Success Rate", False, f"{success_rate:.1f}% success rate ({success_count}/{total_attempts}) - below 90% target")
                return False
                
        except Exception as e:
            self.log_test("Pipeline Success Rate", False, f"Exception: {str(e)}")
            return False
    
    def run_all_tests(self):
        """Run all KE-PR5 versioning tests"""
        print("üéØ KE-PR5 PIPELINE ORCHESTRATOR - STAGE 16 VERSIONING FIX VERIFICATION")
        print("=" * 80)
        print(f"Backend URL: {self.backend_url}")
        print(f"Test Start Time: {datetime.now().isoformat()}")
        print()
        
        # Run all tests
        tests = [
            self.test_v2_engine_availability,
            self.test_simple_content_processing,
            self.test_stage_16_versioning_completion,
            self.test_version_metadata_creation,
            self.test_pipeline_success_rate
        ]
        
        for test in tests:
            try:
                test()
            except Exception as e:
                test_name = test.__name__.replace("test_", "").replace("_", " ").title()
                self.log_test(test_name, False, f"Test exception: {str(e)}")
            
            # Small delay between tests
            time.sleep(2)
        
        # Print summary
        print()
        print("=" * 80)
        print("üéØ KE-PR5 STAGE 16 VERSIONING FIX TEST SUMMARY")
        print("=" * 80)
        
        success_rate = (self.passed_tests / self.total_tests * 100) if self.total_tests > 0 else 0
        
        print(f"Total Tests: {self.total_tests}")
        print(f"Passed: {self.passed_tests}")
        print(f"Failed: {self.total_tests - self.passed_tests}")
        print(f"Success Rate: {success_rate:.1f}%")
        print()
        
        if success_rate >= 90:
            print("üéâ KE-PR5 STAGE 16 VERSIONING FIX: EXCELLENT - All 17 stages completing successfully!")
        elif success_rate >= 80:
            print("‚úÖ KE-PR5 STAGE 16 VERSIONING FIX: GOOD - Most functionality working")
        elif success_rate >= 60:
            print("‚ö†Ô∏è KE-PR5 STAGE 16 VERSIONING FIX: PARTIAL - Some issues remain")
        else:
            print("‚ùå KE-PR5 STAGE 16 VERSIONING FIX: NEEDS ATTENTION - Major issues detected")
        
        print()
        print("Detailed Results:")
        for result in self.test_results:
            status = "‚úÖ" if result["passed"] else "‚ùå"
            print(f"{status} {result['test']}: {result['details']}")
        
        return success_rate

if __name__ == "__main__":
    tester = KEPR5VersioningTester()
    success_rate = tester.run_all_tests()
    
    # Exit with appropriate code
    sys.exit(0 if success_rate >= 90 else 1)