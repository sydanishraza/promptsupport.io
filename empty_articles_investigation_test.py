#!/usr/bin/env python3
"""
CRITICAL BUG INVESTIGATION: Empty Articles Generated by Intelligent Pipeline
Testing all 5 uploaded files to identify patterns and root cause of empty article generation

FILES TO TEST:
1. Customer Summary Screen User Guide 1.3.docx (4.6MB, 85 pages)
2. Promotions Configuration and Management-v5.docx 
3. Google Map JavaScript API Tutorial.docx (user's main concern)
4. Whisk Studio Integration Guide.pdf
5. Filtering Recipes using Custom Labels - Whisk Docs.pdf

INVESTIGATION FOCUS:
- Content Analysis Decision (unified vs split)
- Article Generation Process
- Content Extraction from source documents
- Article Content (empty vs populated)
- Database Storage
- LLM Response Issues
- HTML Cleaning Problems
"""

import requests
import json
import time
import os
import sys
from datetime import datetime
import glob

# Backend URL from frontend .env
BACKEND_URL = "https://1c7fe221-5f78-4e90-bb7b-b530162b68ad.preview.emergentagent.com"
API_BASE = f"{BACKEND_URL}/api"

def log_investigation(message, level="INFO"):
    """Log investigation results with timestamp"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {level}: {message}")

def find_test_files():
    """Find all test files mentioned in the investigation"""
    test_files = []
    
    # Look for files in /app directory
    potential_files = [
        "Customer Summary Screen User Guide 1.3.docx",
        "customer_guide.docx",  # Alternative name
        "Promotions Configuration and Management-v5.docx",
        "Google Map JavaScript API Tutorial.docx",
        "Whisk Studio Integration Guide.pdf",
        "Filtering Recipes using Custom Labels - Whisk Docs.pdf"
    ]
    
    for filename in potential_files:
        file_path = f"/app/{filename}"
        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path)
            test_files.append({
                'name': filename,
                'path': file_path,
                'size': file_size,
                'size_mb': file_size / 1024 / 1024
            })
            log_investigation(f"‚úÖ Found test file: {filename} ({file_size:,} bytes, {file_size/1024/1024:.1f}MB)")
    
    # Also check for any DOCX or PDF files
    docx_files = glob.glob("/app/*.docx")
    pdf_files = glob.glob("/app/*.pdf")
    
    for file_path in docx_files + pdf_files:
        filename = os.path.basename(file_path)
        if not any(f['name'] == filename for f in test_files):
            file_size = os.path.getsize(file_path)
            test_files.append({
                'name': filename,
                'path': file_path,
                'size': file_size,
                'size_mb': file_size / 1024 / 1024
            })
            log_investigation(f"‚úÖ Found additional file: {filename} ({file_size:,} bytes, {file_size/1024/1024:.1f}MB)")
    
    log_investigation(f"üìÅ Total files found for testing: {len(test_files)}")
    return test_files

def get_content_library_baseline():
    """Get baseline article count before testing"""
    try:
        response = requests.get(f"{API_BASE}/content-library", timeout=30)
        if response.status_code == 200:
            data = response.json()
            total_articles = data.get('total', 0)
            log_investigation(f"üìö Content Library baseline: {total_articles} articles")
            return total_articles, data.get('articles', [])
        else:
            log_investigation(f"‚ùå Failed to get Content Library baseline: {response.status_code}")
            return 0, []
    except Exception as e:
        log_investigation(f"‚ùå Error getting Content Library baseline: {e}")
        return 0, []

def analyze_article_content(articles, source_filename):
    """Analyze articles for empty content patterns"""
    analysis = {
        'total_articles': len(articles),
        'empty_articles': 0,
        'populated_articles': 0,
        'html_placeholder_articles': 0,
        'very_short_articles': 0,
        'content_analysis': []
    }
    
    log_investigation(f"üîç ANALYZING {len(articles)} ARTICLES from {source_filename}")
    
    for i, article in enumerate(articles):
        article_id = article.get('id', f'article-{i}')
        title = article.get('title', 'Untitled')
        content = article.get('content', '')
        status = article.get('status', 'unknown')
        
        # Analyze content
        content_length = len(content.strip())
        is_empty = content_length == 0
        has_html_placeholders = '```html' in content or '<!DOCTYPE html>' in content
        is_very_short = content_length < 100 and content_length > 0
        
        article_analysis = {
            'id': article_id,
            'title': title,
            'content_length': content_length,
            'is_empty': is_empty,
            'has_html_placeholders': has_html_placeholders,
            'is_very_short': is_very_short,
            'status': status,
            'content_preview': content[:200] if content else '[EMPTY]'
        }
        
        analysis['content_analysis'].append(article_analysis)
        
        # Update counters
        if is_empty:
            analysis['empty_articles'] += 1
            log_investigation(f"‚ùå EMPTY ARTICLE FOUND: '{title}' (ID: {article_id})")
        elif has_html_placeholders:
            analysis['html_placeholder_articles'] += 1
            log_investigation(f"‚ö†Ô∏è HTML PLACEHOLDER ISSUE: '{title}' (ID: {article_id})")
        elif is_very_short:
            analysis['very_short_articles'] += 1
            log_investigation(f"‚ö†Ô∏è VERY SHORT ARTICLE: '{title}' ({content_length} chars)")
        else:
            analysis['populated_articles'] += 1
            log_investigation(f"‚úÖ POPULATED ARTICLE: '{title}' ({content_length} chars)")
    
    return analysis

def process_file_and_investigate(file_info):
    """Process a single file and investigate the results"""
    filename = file_info['name']
    file_path = file_info['path']
    file_size_mb = file_info['size_mb']
    
    log_investigation(f"üéØ PROCESSING FILE: {filename} ({file_size_mb:.1f}MB)", "CRITICAL")
    
    investigation_results = {
        'filename': filename,
        'file_size_mb': file_size_mb,
        'processing_successful': False,
        'processing_time': 0,
        'articles_generated': 0,
        'content_analysis_decision': 'unknown',
        'article_analysis': None,
        'errors': []
    }
    
    try:
        # Get baseline before processing
        baseline_count, baseline_articles = get_content_library_baseline()
        
        # Upload and process the file
        log_investigation(f"üì§ Uploading {filename}...")
        
        # Determine content type
        content_type = 'application/pdf' if filename.endswith('.pdf') else 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        
        with open(file_path, 'rb') as f:
            files = {'file': (filename, f, content_type)}
            
            start_time = time.time()
            response = requests.post(f"{API_BASE}/content/upload", files=files, timeout=600)
            
            if response.status_code != 200:
                error_msg = f"Upload failed: Status {response.status_code}, Response: {response.text[:500]}"
                investigation_results['errors'].append(error_msg)
                log_investigation(f"‚ùå {error_msg}", "ERROR")
                return investigation_results
            
            upload_data = response.json()
            job_id = upload_data.get('job_id')
            
            if not job_id:
                error_msg = "No job_id received from upload"
                investigation_results['errors'].append(error_msg)
                log_investigation(f"‚ùå {error_msg}", "ERROR")
                return investigation_results
            
            log_investigation(f"‚úÖ Upload successful, Job ID: {job_id}")
        
        # Monitor processing
        log_investigation("‚è≥ Monitoring processing...")
        processing_start = time.time()
        max_wait_time = 600  # 10 minutes
        
        while True:
            elapsed = time.time() - processing_start
            if elapsed > max_wait_time:
                error_msg = f"Processing timeout after {elapsed:.1f} seconds"
                investigation_results['errors'].append(error_msg)
                log_investigation(f"‚ùå {error_msg}", "ERROR")
                return investigation_results
            
            try:
                status_response = requests.get(f"{API_BASE}/jobs/{job_id}", timeout=30)
                if status_response.status_code == 200:
                    status_data = status_response.json()
                    status = status_data.get('status', 'unknown')
                    
                    log_investigation(f"üìä Status: {status} (elapsed: {elapsed:.1f}s)")
                    
                    if status == 'completed':
                        processing_time = time.time() - processing_start
                        investigation_results['processing_time'] = processing_time
                        investigation_results['processing_successful'] = True
                        
                        # Extract metrics
                        chunks_created = status_data.get('chunks_created', 0)
                        articles_generated = status_data.get('articles_generated', 0)
                        investigation_results['articles_generated'] = articles_generated
                        
                        log_investigation(f"‚úÖ Processing completed in {processing_time:.1f} seconds")
                        log_investigation(f"üìà Chunks Created: {chunks_created}")
                        log_investigation(f"üìÑ Articles Generated: {articles_generated}")
                        
                        break
                        
                    elif status == 'failed':
                        error_msg = f"Processing failed: {status_data.get('error', 'Unknown error')}"
                        investigation_results['errors'].append(error_msg)
                        log_investigation(f"‚ùå {error_msg}", "ERROR")
                        return investigation_results
                    
                    time.sleep(10)
                else:
                    log_investigation(f"‚ö†Ô∏è Status check failed: {status_response.status_code}")
                    time.sleep(5)
                    
            except Exception as e:
                log_investigation(f"‚ö†Ô∏è Status check error: {e}")
                time.sleep(5)
        
        # Get updated Content Library and analyze new articles
        log_investigation("üîç Analyzing generated articles...")
        time.sleep(5)  # Wait for database consistency
        
        final_count, final_articles = get_content_library_baseline()
        new_articles_count = final_count - baseline_count
        
        log_investigation(f"üìö Article count change: {baseline_count} ‚Üí {final_count} (+{new_articles_count})")
        
        # Find new articles (those created after baseline)
        new_articles = []
        for article in final_articles:
            if article not in baseline_articles:
                new_articles.append(article)
        
        # If we can't identify new articles by comparison, take the most recent ones
        if len(new_articles) != new_articles_count and articles_generated > 0:
            # Sort by created_at and take the most recent
            sorted_articles = sorted(final_articles, key=lambda x: x.get('created_at', ''), reverse=True)
            new_articles = sorted_articles[:articles_generated]
        
        log_investigation(f"üîç Identified {len(new_articles)} new articles for analysis")
        
        # Analyze the new articles
        if new_articles:
            article_analysis = analyze_article_content(new_articles, filename)
            investigation_results['article_analysis'] = article_analysis
            
            # Determine content analysis decision based on article types
            if len(new_articles) == 1:
                investigation_results['content_analysis_decision'] = 'unified'
            elif len(new_articles) > 1:
                investigation_results['content_analysis_decision'] = 'split'
            
            # Log critical findings
            if article_analysis['empty_articles'] > 0:
                log_investigation(f"üö® CRITICAL ISSUE: {article_analysis['empty_articles']} EMPTY ARTICLES FOUND", "CRITICAL")
            
            if article_analysis['html_placeholder_articles'] > 0:
                log_investigation(f"‚ö†Ô∏è HTML PLACEHOLDER ISSUE: {article_analysis['html_placeholder_articles']} articles with HTML placeholders", "WARNING")
            
            if article_analysis['populated_articles'] > 0:
                log_investigation(f"‚úÖ SUCCESS: {article_analysis['populated_articles']} articles with proper content", "SUCCESS")
        
        return investigation_results
        
    except Exception as e:
        error_msg = f"Investigation failed: {e}"
        investigation_results['errors'].append(error_msg)
        log_investigation(f"‚ùå {error_msg}", "ERROR")
        import traceback
        traceback.print_exc()
        return investigation_results

def run_comprehensive_empty_articles_investigation():
    """Run comprehensive investigation of empty articles bug"""
    log_investigation("üö® STARTING CRITICAL EMPTY ARTICLES BUG INVESTIGATION", "CRITICAL")
    log_investigation("=" * 80)
    
    # Find test files
    test_files = find_test_files()
    
    if not test_files:
        log_investigation("‚ùå No test files found for investigation", "CRITICAL")
        return
    
    # Test backend health first
    log_investigation("üè• Testing backend health...")
    try:
        response = requests.get(f"{API_BASE}/health", timeout=30)
        if response.status_code != 200:
            log_investigation(f"‚ùå Backend health check failed: {response.status_code}", "CRITICAL")
            return
        log_investigation("‚úÖ Backend health check passed")
    except Exception as e:
        log_investigation(f"‚ùå Backend health check failed: {e}", "CRITICAL")
        return
    
    # Process each file and investigate
    investigation_results = []
    
    for i, file_info in enumerate(test_files, 1):
        log_investigation(f"\n{'='*60}")
        log_investigation(f"INVESTIGATION {i}/{len(test_files)}: {file_info['name']}")
        log_investigation(f"{'='*60}")
        
        result = process_file_and_investigate(file_info)
        investigation_results.append(result)
        
        # Wait between tests to avoid overwhelming the system
        if i < len(test_files):
            log_investigation("‚è≥ Waiting 30 seconds before next test...")
            time.sleep(30)
    
    # Generate comprehensive analysis report
    log_investigation(f"\n{'='*80}")
    log_investigation("üéØ COMPREHENSIVE INVESTIGATION RESULTS", "CRITICAL")
    log_investigation(f"{'='*80}")
    
    total_files = len(investigation_results)
    successful_processing = sum(1 for r in investigation_results if r['processing_successful'])
    total_articles_generated = sum(r['articles_generated'] for r in investigation_results)
    
    empty_articles_found = 0
    html_placeholder_issues = 0
    populated_articles = 0
    
    log_investigation(f"üìä OVERALL STATISTICS:")
    log_investigation(f"   Files Tested: {total_files}")
    log_investigation(f"   Successful Processing: {successful_processing}/{total_files}")
    log_investigation(f"   Total Articles Generated: {total_articles_generated}")
    
    # Detailed analysis per file
    for result in investigation_results:
        filename = result['filename']
        log_investigation(f"\nüìÑ FILE: {filename}")
        log_investigation(f"   Size: {result['file_size_mb']:.1f}MB")
        log_investigation(f"   Processing: {'‚úÖ Success' if result['processing_successful'] else '‚ùå Failed'}")
        log_investigation(f"   Processing Time: {result['processing_time']:.1f}s")
        log_investigation(f"   Articles Generated: {result['articles_generated']}")
        log_investigation(f"   Content Decision: {result['content_analysis_decision']}")
        
        if result['article_analysis']:
            analysis = result['article_analysis']
            empty_articles_found += analysis['empty_articles']
            html_placeholder_issues += analysis['html_placeholder_articles']
            populated_articles += analysis['populated_articles']
            
            log_investigation(f"   üìã ARTICLE ANALYSIS:")
            log_investigation(f"      Empty Articles: {analysis['empty_articles']}")
            log_investigation(f"      HTML Placeholder Issues: {analysis['html_placeholder_articles']}")
            log_investigation(f"      Populated Articles: {analysis['populated_articles']}")
            log_investigation(f"      Very Short Articles: {analysis['very_short_articles']}")
            
            # Show details of problematic articles
            for article in analysis['content_analysis']:
                if article['is_empty']:
                    log_investigation(f"      üö® EMPTY: '{article['title']}' (ID: {article['id']})")
                elif article['has_html_placeholders']:
                    log_investigation(f"      ‚ö†Ô∏è HTML ISSUE: '{article['title']}' (ID: {article['id']})")
        
        if result['errors']:
            log_investigation(f"   ‚ùå ERRORS:")
            for error in result['errors']:
                log_investigation(f"      {error}")
    
    # Final summary
    log_investigation(f"\nüéØ CRITICAL FINDINGS SUMMARY:")
    log_investigation(f"   üö® Total Empty Articles Found: {empty_articles_found}")
    log_investigation(f"   ‚ö†Ô∏è HTML Placeholder Issues: {html_placeholder_issues}")
    log_investigation(f"   ‚úÖ Properly Populated Articles: {populated_articles}")
    
    # Determine root cause patterns
    log_investigation(f"\nüîç ROOT CAUSE ANALYSIS:")
    
    if empty_articles_found > 0:
        log_investigation(f"üö® CRITICAL BUG CONFIRMED: {empty_articles_found} empty articles generated", "CRITICAL")
        
        # Analyze patterns
        unified_files = [r for r in investigation_results if r['content_analysis_decision'] == 'unified']
        split_files = [r for r in investigation_results if r['content_analysis_decision'] == 'split']
        
        unified_empty = sum(r['article_analysis']['empty_articles'] for r in unified_files if r['article_analysis'])
        split_empty = sum(r['article_analysis']['empty_articles'] for r in split_files if r['article_analysis'])
        
        log_investigation(f"   Empty articles in unified processing: {unified_empty}")
        log_investigation(f"   Empty articles in split processing: {split_empty}")
        
        if unified_empty > 0 and split_empty > 0:
            log_investigation("   üîç PATTERN: Both unified and split processing affected")
        elif unified_empty > 0:
            log_investigation("   üîç PATTERN: Only unified processing affected")
        elif split_empty > 0:
            log_investigation("   üîç PATTERN: Only split processing affected")
        
        # File type analysis
        docx_files = [r for r in investigation_results if r['filename'].endswith('.docx')]
        pdf_files = [r for r in investigation_results if r['filename'].endswith('.pdf')]
        
        docx_empty = sum(r['article_analysis']['empty_articles'] for r in docx_files if r['article_analysis'])
        pdf_empty = sum(r['article_analysis']['empty_articles'] for r in pdf_files if r['article_analysis'])
        
        log_investigation(f"   Empty articles in DOCX files: {docx_empty}")
        log_investigation(f"   Empty articles in PDF files: {pdf_empty}")
        
    else:
        log_investigation("‚úÖ NO EMPTY ARTICLES FOUND - Bug may be resolved or not reproducible with current files")
    
    if html_placeholder_issues > 0:
        log_investigation(f"‚ö†Ô∏è HTML CLEANING ISSUES: {html_placeholder_issues} articles with HTML placeholder problems")
    
    # Recommendations
    log_investigation(f"\nüí° RECOMMENDATIONS:")
    if empty_articles_found > 0:
        log_investigation("   1. Investigate LLM response handling in create_articles_from_outline()")
        log_investigation("   2. Check clean_article_html_content() function for over-aggressive cleaning")
        log_investigation("   3. Verify content extraction from source documents")
        log_investigation("   4. Add debugging logs to article generation pipeline")
        log_investigation("   5. Implement content validation before database insertion")
    
    if html_placeholder_issues > 0:
        log_investigation("   6. Fix LLM prompting to prevent HTML document structure generation")
        log_investigation("   7. Improve HTML cleaning to remove document tags while preserving content")
    
    return investigation_results

if __name__ == "__main__":
    print("Critical Empty Articles Bug Investigation")
    print("=" * 50)
    
    results = run_comprehensive_empty_articles_investigation()
    
    # Determine exit code based on findings
    if results:
        empty_articles_found = sum(r['article_analysis']['empty_articles'] for r in results if r['article_analysis'])
        if empty_articles_found > 0:
            print(f"\n‚ùå CRITICAL BUG CONFIRMED: {empty_articles_found} empty articles found")
            sys.exit(1)  # Critical bug found
        else:
            print(f"\n‚úÖ NO EMPTY ARTICLES FOUND: Bug not reproduced with current test files")
            sys.exit(0)  # No critical issues
    else:
        print(f"\n‚ùå INVESTIGATION FAILED: Could not complete testing")
        sys.exit(2)  # Investigation failed