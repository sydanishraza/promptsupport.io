name: KE-PR10 Golden Tests & Non-Regression Suite

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
    paths:
      - 'app/engine/**'
      - 'backend/**'
      - 'tests/**'
      - 'requirements.txt'
      - 'pytest.ini'

jobs:
  golden-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
    
    services:
      mongodb:
        image: mongo:5.0
        ports:
          - 27017:27017
        env:
          MONGO_INITDB_ROOT_USERNAME: test
          MONGO_INITDB_ROOT_PASSWORD: test
        options: >-
          --health-cmd mongo
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      with:
        lfs: true  # Enable Git LFS for binary fixtures
        
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
        
    - name: Install System Dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          poppler-utils \
          tesseract-ocr \
          libreoffice \
          pandoc
          
    - name: Install Python Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r backend/requirements.txt
        pip install pytest-cov pytest-asyncio pytest-html pytest-json-report
        
    - name: Setup Test Environment
      run: |
        mkdir -p tests/golden/results
        mkdir -p tests/golden/artifacts
        export MONGO_URL=mongodb://test:test@localhost:27017/promptsupport_test
        
    - name: Run Golden Tests
      id: golden_tests
      run: |
        cd /app
        python -m pytest tests/golden/test_pipeline.py \
          --cov=app \
          --cov-report=html:tests/golden/results/coverage \
          --cov-report=xml:tests/golden/results/coverage.xml \
          --cov-report=term-missing \
          --html=tests/golden/results/report.html \
          --self-contained-html \
          --json-report \
          --json-report-file=tests/golden/results/report.json \
          --junit-xml=tests/golden/results/junit.xml \
          --tb=short \
          -v
      continue-on-error: true
      env:
        MONGO_URL: mongodb://test:test@localhost:27017/promptsupport_test
        PYTHONPATH: /app:/app/backend
        
    - name: Generate Diff Artifacts on Failure
      if: steps.golden_tests.outcome == 'failure'
      run: |
        echo "Generating diff artifacts for failed golden tests..."
        
        # Create diff directory
        mkdir -p tests/golden/artifacts/diffs
        
        # Find and collect diff files generated by test failures
        find tests/golden/expected/ -name "*_diff.*" -exec cp {} tests/golden/artifacts/diffs/ \;
        
        # Generate summary of failures
        echo "# Golden Test Failure Summary" > tests/golden/artifacts/failure-summary.md
        echo "Generated: $(date)" >> tests/golden/artifacts/failure-summary.md
        echo "" >> tests/golden/artifacts/failure-summary.md
        
        # Extract failure details from pytest output
        if [ -f tests/golden/results/report.json ]; then
          python -c "
import json
with open('tests/golden/results/report.json') as f:
    report = json.load(f)
    
print('## Failed Tests')
for test in report.get('tests', []):
    if test.get('outcome') == 'failed':
        print(f'- **{test[\"nodeid\"]}**: {test.get(\"call\", {}).get(\"longrepr\", \"Unknown error\")}')
" >> tests/golden/artifacts/failure-summary.md
        fi
        
    - name: Coverage Threshold Check
      run: |
        python -c "
import xml.etree.ElementTree as ET
import sys

try:
    tree = ET.parse('tests/golden/results/coverage.xml')
    root = tree.getroot()
    
    # Extract coverage percentage
    coverage_elem = root.find('.//coverage')
    if coverage_elem is not None:
        line_rate = float(coverage_elem.get('line-rate', 0))
        coverage_percent = line_rate * 100
        
        print(f'Coverage: {coverage_percent:.1f}%')
        
        # Enforce minimum threshold
        threshold = 80.0  # Minimum coverage percentage
        
        if coverage_percent < threshold:
            print(f'ERROR: Coverage {coverage_percent:.1f}% below threshold {threshold}%')
            sys.exit(1)
        else:
            print(f'✅ Coverage {coverage_percent:.1f}% meets threshold {threshold}%')
    else:
        print('WARNING: Could not parse coverage report')
        
except Exception as e:
    print(f'ERROR: Coverage check failed: {e}')
    sys.exit(1)
"

    - name: Upload Test Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: golden-test-results-py${{ matrix.python-version }}
        path: |
          tests/golden/results/
          tests/golden/artifacts/
        retention-days: 30
        
    - name: Upload Coverage to Codecov
      if: matrix.python-version == '3.10'
      uses: codecov/codecov-action@v3
      with:
        file: tests/golden/results/coverage.xml
        flags: golden-tests
        name: golden-tests-coverage
        fail_ci_if_error: false
        
    - name: Comment PR with Test Results
      if: github.event_name == 'pull_request' && matrix.python-version == '3.10'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          // Read test results
          let summary = '## 🧪 Golden Tests Results\n\n';
          
          try {
            const reportPath = 'tests/golden/results/report.json';
            if (fs.existsSync(reportPath)) {
              const report = JSON.parse(fs.readFileSync(reportPath, 'utf8'));
              
              const total = report.summary?.total || 0;
              const passed = report.summary?.passed || 0;
              const failed = report.summary?.failed || 0;
              const success_rate = total > 0 ? ((passed / total) * 100).toFixed(1) : 0;
              
              summary += `**Test Summary:**\n`;
              summary += `- Total Tests: ${total}\n`;
              summary += `- Passed: ${passed} ✅\n`;
              summary += `- Failed: ${failed} ❌\n`;
              summary += `- Success Rate: ${success_rate}%\n\n`;
              
              if (failed > 0) {
                summary += `**❌ Failed Tests:** Check artifacts for detailed diff analysis\n\n`;
              }
              
              // Coverage info
              const coveragePath = 'tests/golden/results/coverage.xml';
              if (fs.existsSync(coveragePath)) {
                summary += `**Coverage:** See detailed report in artifacts\n\n`;
              }
              
              summary += `**📊 Regression Protection:**\n`;
              summary += `- Coverage threshold enforced: >= 80%\n`;
              summary += `- Anchor stability validated\n`;
              summary += `- Performance benchmarks checked\n`;
              summary += `- Quality gates: ${failed === 0 ? 'PASSED ✅' : 'FAILED ❌'}\n`;
              
            } else {
              summary += '⚠️ Test report not found - check workflow logs\n';
            }
          } catch (error) {
            summary += `⚠️ Error reading test results: ${error.message}\n`;
          }
          
          // Post comment
          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });
          
    - name: Fail Build on Test Failures
      if: steps.golden_tests.outcome == 'failure'
      run: |
        echo "❌ Golden tests failed - check artifacts for detailed analysis"
        echo "To update baselines (if changes are intentional):"
        echo "  pytest tests/golden/test_pipeline.py --update-golden"
        exit 1

  baseline-drift-check:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Checkout PR Branch
      uses: actions/checkout@v4
      with:
        ref: ${{ github.head_ref }}
        
    - name: Checkout Base Branch  
      uses: actions/checkout@v4
      with:
        ref: ${{ github.base_ref }}
        path: base-branch
        
    - name: Check for Baseline Changes
      run: |
        echo "🔍 Checking for golden baseline drift..."
        
        # Compare baseline files between branches
        changed_baselines=$(git diff --name-only HEAD base-branch/HEAD -- tests/golden/expected/ || true)
        
        if [ -n "$changed_baselines" ]; then
          echo "⚠️ Golden baseline files changed:"
          echo "$changed_baselines" | sed 's/^/  - /'
          
          echo ""
          echo "**Baseline Update Checklist:**"
          echo "- [ ] Changes are intentional and approved"
          echo "- [ ] Commit message explains rationale for baseline updates" 
          echo "- [ ] QA metrics remain within acceptable ranges"
          echo "- [ ] Anchor IDs remain stable (no unintended drift)"
          echo ""
          echo "If changes are unintentional, review recent code modifications."
          echo "If changes are intentional, ensure proper documentation."
          
          # Set output for PR comment
          echo "baseline_changes=true" >> $GITHUB_OUTPUT
          echo "changed_files<<EOF" >> $GITHUB_OUTPUT
          echo "$changed_baselines" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
        else
          echo "✅ No golden baseline changes detected"
          echo "baseline_changes=false" >> $GITHUB_OUTPUT
        fi
      id: baseline_check
      
    - name: Comment on Baseline Changes
      if: steps.baseline_check.outputs.baseline_changes == 'true'
      uses: actions/github-script@v6
      with:
        script: |
          const changedFiles = `${{ steps.baseline_check.outputs.changed_files }}`;
          
          const comment = `## ⚠️ Golden Baseline Changes Detected
          
The following golden test baseline files have been modified:

${changedFiles.split('\n').map(f => f.trim()).filter(f => f).map(f => `- \`${f}\``).join('\n')}

### 📋 Review Checklist

Before merging this PR, please ensure:

- [ ] **Intentional Changes**: All baseline modifications are intentional and approved
- [ ] **Documentation**: Commit message clearly explains the rationale for baseline updates
- [ ] **Quality Validation**: QA metrics (coverage, issues) remain within acceptable ranges  
- [ ] **Anchor Stability**: Heading IDs and cross-references remain stable
- [ ] **Testing**: Updated baselines produce expected results in all environments

### 🔄 Updating Baselines

If you need to update baselines for intentional changes:

\`\`\`bash
# Update all baselines
pytest tests/golden/test_pipeline.py --update-golden

# Update specific baseline  
pytest tests/golden/test_pipeline.py::test_golden_markdown_pipeline --update-golden
\`\`\`

### ❓ Questions?

If these changes are unintentional, review recent modifications to:
- V2 processing pipeline (\`app/engine/v2/\`)
- Repository pattern (\`app/engine/stores/\`)
- Content processing logic (\`backend/server.py\`)

For help with golden tests, see: \`tests/golden/README.md\`
`;
          
          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

env:
  PYTHONPATH: /app:/app/backend
  NODE_ENV: test