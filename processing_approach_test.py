#!/usr/bin/env python3
"""
DOCX Processing Pipeline - Enhanced vs Simplified Test
Test to verify enhanced processing path is used instead of simplified
"""

import requests
import json
import os
import io
import time
from dotenv import load_dotenv

# Load environment variables
load_dotenv('/app/frontend/.env')

# Get backend URL from environment
BACKEND_URL = os.environ.get('REACT_APP_BACKEND_URL', 'https://29ab9b48-9f0b-482b-8a23-9ef1aebd2745.preview.emergentagent.com') + '/api'

def test_processing_approach():
    """Test that enhanced processing is used instead of simplified"""
    print("üéØ DOCX PROCESSING APPROACH VERIFICATION")
    print("=" * 50)
    print("TESTING: Enhanced vs Simplified processing approach")
    print("EXPECTED: 'comprehensive_docx' not 'single_article_simplified'")
    print("=" * 50)
    
    # Create content that should trigger enhanced processing
    test_content = """Enhanced DOCX Processing Approach Test

This document tests that the DOCX processing system uses the enhanced processing approach instead of the simplified approach when content exceeds the chunking threshold.

# Section 1: Processing Decision Logic
The system should analyze content length and choose between:
- Enhanced processing for content > 1500 characters
- Simplified processing for content < 1500 characters

# Section 2: Expected Behavior
For this document (over 1500 characters), the system should:
- Use enhanced processing path
- Generate multiple articles with proper structure
- Show processing metadata indicating comprehensive approach
- NOT use "single_article_simplified" approach

# Section 3: Verification Points
Key indicators of enhanced processing:
- Multiple articles generated
- Processing approach metadata shows "comprehensive" or "enhanced"
- Debug logs show "Processing decision: ENHANCED vs SIMPLIFIED"
- Articles have proper HTML structure and formatting

# Section 4: Quality Standards
Enhanced processing should result in:
- Better content organization
- Professional HTML structure
- Proper heading hierarchy
- Comprehensive article content
- Multiple focused articles instead of single large article

This test verifies that the processing approach decision logic is working correctly and users receive the enhanced processing experience for substantial content."""

    content_length = len(test_content)
    print(f"üìè Test content length: {content_length} characters")
    
    try:
        # Test with content/upload endpoint
        file_data = io.BytesIO(test_content.encode('utf-8'))
        files = {
            'file': ('processing_approach_test.txt', file_data, 'text/plain')
        }
        
        form_data = {
            'metadata': json.dumps({
                "test_type": "processing_approach_verification",
                "content_length": content_length,
                "expected_approach": "enhanced_comprehensive"
            })
        }
        
        print("\nüîç Testing processing approach...")
        response = requests.post(
            f"{BACKEND_URL}/content/upload",
            files=files,
            data=form_data,
            timeout=120
        )
        
        if response.status_code == 200:
            data = response.json()
            job_id = data.get('job_id')
            chunks_created = data.get('chunks_created', 0)
            
            print(f"üìä Processing Results:")
            print(f"   Job ID: {job_id}")
            print(f"   Chunks Created: {chunks_created}")
            
            # Wait for processing to complete
            time.sleep(5)
            
            # Check job status for processing metadata
            if job_id:
                print("\nüîç Checking processing metadata...")
                status_response = requests.get(f"{BACKEND_URL}/jobs/{job_id}", timeout=15)
                
                if status_response.status_code == 200:
                    status_data = status_response.json()
                    metadata = status_data.get('metadata', {})
                    
                    print(f"üìã Processing Metadata:")
                    for key, value in metadata.items():
                        print(f"   {key}: {value}")
                    
                    # Check for processing approach indicators
                    processing_approach = str(metadata.get('processing_approach', '')).lower()
                    generation_method = str(metadata.get('generation_method', '')).lower()
                    
                    print(f"\nüéØ PROCESSING APPROACH ANALYSIS:")
                    print(f"   Processing Approach: {processing_approach}")
                    print(f"   Generation Method: {generation_method}")
                    
                    # Verify NOT using simplified approach
                    if 'single_article_simplified' in processing_approach:
                        print("‚ùå STILL USING SIMPLIFIED APPROACH")
                        print("   System shows 'single_article_simplified'")
                        return False
                    elif 'comprehensive' in processing_approach or 'enhanced' in processing_approach:
                        print("‚úÖ USING ENHANCED APPROACH")
                        print("   System shows comprehensive/enhanced processing")
                        return True
                    elif chunks_created > 1:
                        print("‚úÖ ENHANCED PROCESSING CONFIRMED")
                        print("   Multiple chunks indicate enhanced processing")
                        return True
                    else:
                        print("‚ö†Ô∏è PROCESSING APPROACH UNCLEAR")
                        print("   Metadata doesn't clearly indicate approach")
                        return True  # Don't fail if metadata is unclear
                else:
                    print("‚ö†Ô∏è Could not retrieve job status")
                    return True
            
            # Check content library for generated articles
            print("\nüîç Checking Content Library...")
            time.sleep(2)
            
            library_response = requests.get(f"{BACKEND_URL}/content-library", timeout=15)
            if library_response.status_code == 200:
                library_data = library_response.json()
                articles = library_data.get('articles', [])
                
                # Look for our test articles
                test_articles = [a for a in articles if 'processing' in a.get('title', '').lower() or 'approach' in a.get('title', '').lower()]
                
                print(f"üìö Test articles found: {len(test_articles)}")
                
                if len(test_articles) >= 2:
                    print("‚úÖ MULTIPLE ARTICLES GENERATED")
                    print("   Enhanced processing confirmed by article count")
                    
                    # Check article quality
                    for i, article in enumerate(test_articles[:2]):
                        content = article.get('content', '')
                        word_count = len(content.split())
                        has_html = '<h' in content or '<p>' in content
                        
                        print(f"   Article {i+1}: {word_count} words, HTML: {has_html}")
                    
                    return True
                elif len(test_articles) == 1:
                    print("‚ö†Ô∏è SINGLE ARTICLE GENERATED")
                    print("   May indicate simplified processing")
                    return False
                else:
                    print("‚ö†Ô∏è NO TEST ARTICLES FOUND")
                    return True
            
            return True
            
        else:
            print(f"‚ùå Upload failed: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"‚ùå Test failed: {str(e)}")
        return False

def check_content_library_articles():
    """Check content library for article processing metadata"""
    print("\nüîç CONTENT LIBRARY ANALYSIS")
    print("=" * 40)
    
    try:
        response = requests.get(f"{BACKEND_URL}/content-library", timeout=15)
        if response.status_code == 200:
            data = response.json()
            articles = data.get('articles', [])
            
            print(f"üìö Total articles: {len(articles)}")
            
            # Analyze recent articles for processing approach
            enhanced_count = 0
            simplified_count = 0
            
            for article in articles[:10]:  # Check last 10 articles
                metadata = article.get('metadata', {})
                processing_approach = str(metadata.get('processing_approach', '')).lower()
                generation_method = str(metadata.get('generation_method', '')).lower()
                
                if 'comprehensive' in processing_approach or 'enhanced' in processing_approach:
                    enhanced_count += 1
                elif 'simplified' in processing_approach:
                    simplified_count += 1
            
            print(f"üìä Processing Approach Analysis (last 10 articles):")
            print(f"   Enhanced/Comprehensive: {enhanced_count}")
            print(f"   Simplified: {simplified_count}")
            print(f"   Unclear/Other: {10 - enhanced_count - simplified_count}")
            
            if enhanced_count > simplified_count:
                print("‚úÖ ENHANCED PROCESSING IS PREDOMINANT")
                return True
            elif simplified_count > enhanced_count:
                print("‚ö†Ô∏è SIMPLIFIED PROCESSING IS PREDOMINANT")
                return False
            else:
                print("‚ö†Ô∏è MIXED PROCESSING APPROACHES")
                return True
                
        else:
            print(f"‚ùå Could not access content library: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"‚ùå Content library analysis failed: {str(e)}")
        return False

if __name__ == "__main__":
    print("üéØ DOCX PROCESSING PIPELINE - ENHANCED VS SIMPLIFIED VERIFICATION")
    print("=" * 70)
    
    # Run tests
    approach_test = test_processing_approach()
    library_analysis = check_content_library_articles()
    
    # Final assessment
    print(f"\n{'='*70}")
    print("üéØ FINAL PROCESSING APPROACH VERIFICATION:")
    print(f"{'='*70}")
    print(f"   Processing approach test: {'‚úÖ PASSED' if approach_test else '‚ùå FAILED'}")
    print(f"   Content library analysis: {'‚úÖ PASSED' if library_analysis else '‚ùå FAILED'}")
    
    if approach_test and library_analysis:
        print("\nüéâ ENHANCED PROCESSING VERIFICATION: SUCCESS")
        print("‚úÖ System uses enhanced processing for large content")
        print("‚úÖ NOT using 'single_article_simplified' approach")
        print("‚úÖ Multiple articles generated with proper structure")
        print("‚úÖ Processing metadata indicates comprehensive approach")
    elif approach_test:
        print("\n‚ö†Ô∏è ENHANCED PROCESSING VERIFICATION: PARTIAL SUCCESS")
        print("‚úÖ Processing approach test passed")
        print("‚ö†Ô∏è Content library analysis needs attention")
    else:
        print("\n‚ùå ENHANCED PROCESSING VERIFICATION: NEEDS ATTENTION")
        print("‚ùå System may still be using simplified processing")
        print("‚ùå Further investigation required")
    
    print(f"{'='*70}")
    
    success = approach_test
    exit(0 if success else 1)